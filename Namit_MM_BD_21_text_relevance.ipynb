{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Намит Максим BD-21. Ранжирование документов по текстовой релевантности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для соревнования достаточно хаотичен, в данном файле привожу некоторую его краткую \n",
    "выжимку (некоторые служебные части кода: запись в файл, чтение из файла чего-либо могут быть пропущены)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Обработка запросов. Спеллер от яндекса + нормализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaspeller import YandexSpeller\n",
    "from tqdm.notebook import tqdm\n",
    "from pyaspeller import Word\n",
    "import pymorphy2\n",
    "import gzip\n",
    "from multiprocessing.dummy import  Queue\n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellechecker = YandexSpeller(lang='ru', find_repeat_words=False, \\\n",
    "                              ignore_digits=True, max_requests=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_q = []\n",
    "with open(\"queries.tsv\", 'r', encoding = 'utf-8') as f:\n",
    "    for q in f:\n",
    "        bad_q.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправление запросов с помощью спеллера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, q in tqdm(enumerate(bad_q)):\n",
    "    fix = spellechecker.spelled(q)\n",
    "    for word in fix.split():\n",
    "        word_check = Word(word)\n",
    "        if not word_check.correct:\n",
    "            fix = fix[:-1] + ' !!!!!!!!!!!!!!!!!' + '\\n'\n",
    "            break\n",
    "    fixes.append(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"first_fixes.txt\", 'w', encoding = 'utf-8') as f:\n",
    "    for q in fixes:\n",
    "        f.write(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_normal(query):\n",
    "    els = query.split('\\t')\n",
    "    token = word_tokenize(els[1])\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in token]\n",
    "    normal_query = els[0] + '\\t' + ' '.join(lemmas) + '\\n'\n",
    "    return [normal_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(queries)) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    with Pool(processes=4) as pool:\n",
    "        for i, el in enumerate(pool.imap_unordered(to_normal, queries)):\n",
    "            with lock:\n",
    "                norm_queries.extend(el)\n",
    "                pbar.update(1)\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обработка данных : выгрузка данных, нормализация заголовков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По частям выгружаю файлы в txt формат (вроде брал по 100.000), затем архивирую их а исходные txt удаляю, иначе не хватало места на диске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('docs.tsv.gz', 'rb') as f:\n",
    "    i = 0\n",
    "    line = f.readline()\n",
    "    while (i < 107045):\n",
    "        line=f.readline()\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            clear_output()\n",
    "            print('Doc i =', i, ' ALL = 582166')\n",
    "    while (line):\n",
    "        els = line.decode('utf8', 'ignore').split('\\t')\n",
    "        file = open('docs/'+els[0]+'.txt', \"w\")\n",
    "        file.write(els[1] + '\\n\\t' + els[2])\n",
    "        file.close()\n",
    "        line=f.readline()\n",
    "        i+=1\n",
    "        if i > 107045:\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            clear_output()\n",
    "            print('Doc i =', i, ' ALL = 582166')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(i):\n",
    "    file = open('docs/'+str(i)+'.txt', \"r\")\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    os.remove('docs/'+str(i)+'.txt')\n",
    "    byte_data = (content).encode('utf-8')\n",
    "    with gzip.open('docs/'+str(i)+'.txt.gz', \"wb\") as file:\n",
    "        file.write(byte_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(queue)) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    with Pool(processes=4) as pool:\n",
    "        for i, _ in enumerate(pool.imap_unordered(process, queue)):\n",
    "            with lock:\n",
    "                pbar.update(1)\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгружаем номера нужных документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_doc = {}\n",
    "with open(\"sample.csv\", 'r') as f:\n",
    "    f.readline()\n",
    "    for line in tqdm(f.readlines()):\n",
    "        split = line.split(',')\n",
    "        if int(split[0]) not in queries_doc.keys():\n",
    "            queries_doc[int(split[0])]=[int(split[1])]\n",
    "        else:\n",
    "            queries_doc[int(split[0])].append(int(split[1]))\n",
    "for i in queries_doc:\n",
    "    queries_doc[i] = sorted(queries_doc[i])\n",
    "docs = set()\n",
    "for key in queries_doc:\n",
    "    for doc in queries_doc[key]:\n",
    "        docs.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгружаем нужные заголовки для их последующей нормализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(i):\n",
    "    temp_titles = []\n",
    "    temp_bad = []\n",
    "    temp_bad_len = []\n",
    "    with gzip.open('docs/'+str(i)+'.txt.gz', \"r\") as file:\n",
    "        lines = file.read()\n",
    "        els = lines.split(b'\\t')\n",
    "        if (len(els) < 2):\n",
    "            temp_bad.append(i)\n",
    "        title = str(i) + '\\t' + els[0][:-1].decode('utf8', 'ignore').lower()\n",
    "        if (len(title) > 500):\n",
    "            temp_bad_len.append(i)\n",
    "        temp_titles.append(title)\n",
    "    return temp_titles, temp_bad, temp_bad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "bad = []\n",
    "bad_len = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(queue)) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    with Pool(processes=4) as pool:\n",
    "        for i, el in enumerate(pool.imap_unordered(get_titles, queue)):\n",
    "            with lock:\n",
    "                titles.extend(el[0])\n",
    "                bad.extend(el[1])\n",
    "                bad_len.extend(el[2])\n",
    "                pbar.update(1)\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация заголовков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_normal(title):\n",
    "    els = title.split('\\t')\n",
    "    token = word_tokenize(els[1])\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in token]\n",
    "    normal_title = els[0] + '\\t' + ' '.join(lemmas) + '\\n'\n",
    "    return [normal_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(queries)) as pbar:\n",
    "    lock = pbar.get_lock()\n",
    "    with Pool(processes=4) as pool:\n",
    "        for i, el in enumerate(pool.imap_unordered(to_normal, queries)):\n",
    "            with lock:\n",
    "                norm_queries.extend(el)\n",
    "                pbar.update(1)\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_titles_norm = dict()\n",
    "for title in norm_titles:\n",
    "    els = title.split('\\t')\n",
    "    doc_titles_norm[int(els[0])] = els[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titles_norm_1.txt', 'w') as f:\n",
    "    for i in sorted(doc_titles_norm.keys()):\n",
    "        f.write(str(i) + '\\t' + doc_titles_norm[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_doc = {}\n",
    "with open(\"sample.csv\", 'r') as f:\n",
    "    f.readline()\n",
    "    for line in tqdm(f.readlines()):\n",
    "        split = line.split(',')\n",
    "        if int(split[0]) not in queries_doc.keys():\n",
    "            queries_doc[int(split[0])]=[int(split[1])]\n",
    "        else:\n",
    "            queries_doc[int(split[0])].append(int(split[1]))\n",
    "for i in queries_doc:\n",
    "    queries_doc[i] = sorted(queries_doc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "# здесь также мог быть другой файл, в котором содержались не нормализованные запросы,\n",
    "# в итоге лучший результат был получен с помощью не нормализованных запросов и заголовков\n",
    "with open(\"norm_queries.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        queries.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_queries = {}\n",
    "for query in queries:\n",
    "    els = query.split('\\t')\n",
    "    doc_queries[int(els[0])] = els[1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если используем нормализованные заголовки их надо прочитать\n",
    "norm_titles = {}\n",
    "with open(\"titles_norm_1.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        els = line.split('\\t')\n",
    "        norm_titles[int(els[0])] = els[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель на заголовках - \"грубый\" результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(sorted(queries_doc.keys())):\n",
    "    try:\n",
    "        titles = []\n",
    "        texts = []\n",
    "        nums = []\n",
    "        nums_titles = {}\n",
    "        size = 0\n",
    "        for doc in queries_doc[i]:\n",
    "            # опять же можно использовать как нормализованные так и не нормализованные\n",
    "\n",
    "            #with gzip.open('docs/'+str(doc)+'.txt.gz', \"r\") as file:\n",
    "            #    text = file.read().split(b'\\t')\n",
    "            #    if len(text) < 2:\n",
    "            #        continue\n",
    "            #    title = text[0].decode('utf8', 'ignore')\n",
    "            title = norm_titles[doc][:-1]\n",
    "            if len(title) > 1000:\n",
    "                title = title[:1000]\n",
    "            titles.append(title)\n",
    "            nums.append(doc)\n",
    "        question_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant([queries[i].split('\\t')[1].lower()])\n",
    "        )\n",
    "        encodings = module.signatures['response_encoder'](\n",
    "            input=tf.constant(titles),\n",
    "            context=tf.constant(titles)\n",
    "        )\n",
    "        scores = np.inner(question_embeddings['outputs'], encodings['outputs'])\n",
    "        tmp = {}\n",
    "        for j in range(len(titles)):\n",
    "            tmp[nums[j]] = scores[0, j]\n",
    "        result[i] = sorted(tmp.items(), key = lambda x: x[1], reverse=True)\n",
    "    except:\n",
    "        bad.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "with open('baseline_norm_titles.csv','w') as f:\n",
    "    f.write('QueryId,DocumentId\\n')\n",
    "    for i in sorted(result.keys()):\n",
    "        temp = result[i][:10]\n",
    "        if (len(temp) != 10):\n",
    "            print('!!!!!', i)\n",
    "        for item in temp:\n",
    "            f.write('{},{}\\n'.format(i,item[0]))\n",
    "            k += 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем контекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rough_result.txt', 'w') as f:\n",
    "    for i in result:\n",
    "        for j in result[i]:\n",
    "            f.write(str(i) + ',' + str(j[0]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_30 = {}\n",
    "with open(\"rough_result.txt\", 'r') as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        split = line.split(',')\n",
    "        if int(split[0]) not in result_30.keys():\n",
    "            result_30[int(split[0])]=[int(split[1])]\n",
    "        else:\n",
    "            result_30[int(split[0])].append(int(split[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По 500-700 запросов для надежности (исполнение иногда падало). Лучше всего сработало именно на 30 документах. То есть имеем результат ранжирования на заголовках и переранжируем его с учетом документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(sorted(result_30.keys())[0:500]):\n",
    "    try:\n",
    "        titles = []\n",
    "        texts = []\n",
    "        nums = []\n",
    "        nums_titles = {}\n",
    "        size = 0\n",
    "        for doc in result_30[i]:\n",
    "            with gzip.open('docs/'+str(doc)+'.txt.gz', \"r\") as file:\n",
    "                els = file.read().split(b'\\t')\n",
    "                if len(els) < 2:\n",
    "                    continue\n",
    "                title = els[0].decode('utf8', 'ignore')\n",
    "                if len(title) > 500:\n",
    "                    title = title[0:500]\n",
    "                text = els[1].decode('utf8', 'ignore')\n",
    "            titles.append(title[:-1].lower())\n",
    "            texts.append(text.lower())\n",
    "            nums.append(doc)\n",
    "            nums_titles[doc] = title[:-1].lower()\n",
    "        question_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant([queries[i].split('\\t')[1].lower()])\n",
    "        )\n",
    "        encodings = module.signatures['response_encoder'](\n",
    "            input=tf.constant(titles),\n",
    "            context=tf.constant(texts)\n",
    "        )\n",
    "        scores = np.inner(question_embeddings['outputs'], encodings['outputs'])\n",
    "        tmp = {}\n",
    "        for j in range(len(titles)):\n",
    "            tmp[nums[j]] = scores[0, j]\n",
    "        result[i] = sorted(tmp.items(), key = lambda x: x[1], reverse=True)\n",
    "        with open('sub.txt', 'a') as f:\n",
    "            f.write(str(i) + '\\t ')\n",
    "            for j in result[i]:\n",
    "                f.write(str(j[0]) + ' ')\n",
    "            f.write('\\n')\n",
    "    except:\n",
    "        bad.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробовал увеличивать число документов - не помогло (возможно криво сделал, потому что разбил каждый запрос на 3 части чтобы влезло в RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(sorted(result_30.keys())[314:]):\n",
    "    tmp = {}\n",
    "    for step in (range(3)):     \n",
    "        titles = []\n",
    "        texts = []\n",
    "        nums = []\n",
    "        nums_titles = {}\n",
    "        size = 0\n",
    "        start = int(step * len(result_30[i]) / 3)\n",
    "        end   = int((step + 1) * len(result_30[i]) / 3 if step != 2 else -1)\n",
    "        if end != -1:\n",
    "            docs_part = result_30[i][start:end]\n",
    "        else:\n",
    "            docs_part = result_30[i][start:]\n",
    "        for doc in docs_part:\n",
    "            with gzip.open('docs/'+str(doc)+'.txt.gz', \"r\") as file:\n",
    "                els = file.read().split(b'\\t')\n",
    "                if len(els) < 2:\n",
    "                    continue\n",
    "                title = els[0].decode('utf8', 'ignore')\n",
    "                if len(title) > 500:\n",
    "                    title = title[0:500]\n",
    "                text = els[1].decode('utf8', 'ignore')\n",
    "            titles.append(title[:-1].lower())\n",
    "            texts.append(text.lower())\n",
    "            nums.append(doc)\n",
    "            nums_titles[doc] = title[:-1].lower()\n",
    "        question_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant([queries[i].split('\\t')[1].lower()])\n",
    "        )\n",
    "        encodings = module.signatures['response_encoder'](\n",
    "            input=tf.constant(titles),\n",
    "            context=tf.constant(texts)\n",
    "        )\n",
    "        scores = np.inner(question_embeddings['outputs'], encodings['outputs'])\n",
    "        for j in range(len(titles)):\n",
    "            tmp[nums[j]] = scores[0, j]\n",
    "    result[i] = sorted(tmp.items(), key = lambda x: x[1], reverse=True)\n",
    "    with open('final.txt', 'a') as f:\n",
    "        f.write(str(i) + '\\t ')\n",
    "        for j in result[i]:\n",
    "            f.write(str(j[0]) + ' ')\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
